{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_learning_TransferLearning.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ajiKLOuSx2e"
      },
      "source": [
        "# Transfer learning & fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOR8ZV8DT5k0"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrMEfvGtS83U"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-jr9SXkYj13"
      },
      "source": [
        "##Introduction\n",
        "Transfer learning consists of taking features learned on one problem, and leveraging them on a new, similar problem.\n",
        "\n",
        "Transfer learning is usually done for tasks where your dataset has too little data to train a full-scale model from scratch.\n",
        "\n",
        "The most common incarnation of transfer learning in the context of deep learning is the following workflow:\n",
        "\n",
        "1-Take layers from a previously trained model.<br/>\n",
        "2-Freeze them, so as to avoid destroying any of the information they contain during future training rounds.<br/>\n",
        "3-Add some new, trainable layers on top of the frozen layers. They will learn to turn the old features into predictions on a new dataset.<br/>\n",
        "4-rain the new layers on your dataset.<br/>\n",
        "\n",
        "-->A last, optional step, is fine-tuning, which consists of unfreezing the entire model you obtained above (or part of it), and re-training it on the new data with a very low learning rate. This can potentially achieve meaningful improvements, by incrementally adapting the pretrained features to the new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf8XkIzyZA31"
      },
      "source": [
        "## fine-tuning an image classification model on a cats vs. dogs dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS_oqYXaZKzO"
      },
      "source": [
        "### Getting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCILemZJZIVA"
      },
      "source": [
        "tfds.disable_progress_bar()\n",
        "\n",
        "train_ds, validation_ds, test_ds = tfds.load(\n",
        "    \"cats_vs_dogs\",\n",
        "    # Reserve 10% for validation and 10% for test\n",
        "    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n",
        "    as_supervised=True,  # Include labels\n",
        ")\n",
        "\n",
        "print(\"Number of training samples: %d\" % tf.data.experimental.cardinality(train_ds))\n",
        "print(\n",
        "    \"Number of validation samples: %d\" % tf.data.experimental.cardinality(validation_ds)\n",
        ")\n",
        "print(\"Number of test samples: %d\" % tf.data.experimental.cardinality(test_ds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J05M4caaQRv"
      },
      "source": [
        "These are the first 9 images in the training dataset -- as you can see, they're all different sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEOJFQpIabZO"
      },
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for i, (image, label) in enumerate(train_ds.take(9)):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(int(label))\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSLm-Qwaanp-"
      },
      "source": [
        "### Standardizing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXsrpxkDatYW"
      },
      "source": [
        "Let's resize images to 150x150:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpkF5HZPaqS3"
      },
      "source": [
        "size = (150, 150)\n",
        "\n",
        "train_ds = train_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
        "validation_ds = validation_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
        "test_ds = test_ds.map(lambda x, y: (tf.image.resize(x, size), y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfFoUW5Oaywq"
      },
      "source": [
        "Besides, let's batch the data and use caching & prefetching to optimize loading speed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMwItv30axuY"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_ds = train_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
        "validation_ds = validation_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().batch(batch_size).prefetch(buffer_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63KQzeEda8Q5"
      },
      "source": [
        "### Using random data augmentation\n",
        "When you don't have a large image dataset, it's a good practice to artificially introduce sample diversity by applying random yet realistic transformations to the training images, such as random horizontal flipping or small random rotations. This helps expose the model to different aspects of the training data while slowing down overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW19XBS_bFih"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "        layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfM2TecobSXY"
      },
      "source": [
        "Let's visualize what the first image of the first batch looks like after various random transformations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppre7AYsbTjR"
      },
      "source": [
        "for images, labels in train_ds.take(1):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    first_image = images[0]\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        augmented_image = data_augmentation(\n",
        "            tf.expand_dims(first_image, 0), training=True\n",
        "        )\n",
        "        plt.imshow(augmented_image[0].numpy().astype(\"int32\"))\n",
        "        plt.title(int(labels[i]))\n",
        "        plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoAA-FZ8ba99"
      },
      "source": [
        "### Build a model\n",
        "-We add a Normalization layer to scale input values (initially in the [0, 255] range) to the [-1, 1] range.<br/>\n",
        "-We add a Dropout layer before the classification layer, for regularization.<br/>\n",
        "-We make sure to pass training=False when calling the base model, so that it runs in inference mode, so that batchnorm statistics don't get updated even after we unfreeze the base model for fine-tuning.<br/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apznLeqqbg9A"
      },
      "source": [
        "base_model = keras.applications.Xception(\n",
        "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
        "    input_shape=(150, 150, 3),\n",
        "    include_top=False,\n",
        ")  # Do not include the ImageNet classifier at the top.\n",
        "\n",
        "# Freeze the base_model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create new model on top\n",
        "inputs = keras.Input(shape=(150, 150, 3))\n",
        "x = data_augmentation(inputs)  # Apply random data augmentation\n",
        "\n",
        "# Pre-trained Xception weights requires that input be normalized\n",
        "# from (0, 255) to a range (-1., +1.), the normalization layer\n",
        "# does the following, outputs = (inputs - mean) / sqrt(var)\n",
        "norm_layer = keras.layers.experimental.preprocessing.Normalization()\n",
        "mean = np.array([127.5] * 3)\n",
        "var = mean ** 2\n",
        "# Scale inputs to [-1, +1]\n",
        "x = norm_layer(x)\n",
        "norm_layer.set_weights([mean, var])\n",
        "\n",
        "# The base model contains batchnorm layers. We want to keep them in inference mode\n",
        "# when we unfreeze the base model for fine-tuning, so we make sure that the\n",
        "# base_model is running in inference mode here.\n",
        "x = base_model(x, training=False)\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
        "outputs = keras.layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-PUMT_vbqVY"
      },
      "source": [
        "### Train the top layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXuG0oUGbrdR"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.BinaryAccuracy()],\n",
        ")\n",
        "\n",
        "epochs = 20\n",
        "model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj17I7rndv33"
      },
      "source": [
        "# Unfreeze the base_model. Note that it keeps running in inference mode\n",
        "# since we passed `training=False` when calling it. This means that\n",
        "# the batchnorm layers will not update their batch statistics.\n",
        "# This prevents the batchnorm layers from undoing all the training\n",
        "# we've done so far.\n",
        "base_model.trainable = True\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate\n",
        "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.BinaryAccuracy()],\n",
        ")\n",
        "\n",
        "epochs = 10\n",
        "model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}